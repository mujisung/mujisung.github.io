---
layout: posts
title: CreateYourWorldSunEtAl2023 
categories: [thesis, AI]
tag: [thesis_name  ]  
author_profile: false # ê¸€ì„ ëˆ„ë¥´ë©´ ë‚´ ì†Œê°œê°€ ì—†ì–´ì§. trueë¡œ í•˜ë©´ ì–¼êµ´ì´ ë‚˜ì˜´.
sidebar:      # ê¸€ì„ ëˆ„ë¥´ë©´ ëª©ì°¨ê°€ ë‚˜ì˜¨ë‹¤.
  nav: "counts" 
search: true     # falseë¼ê³  í•˜ë©´ ê¸€ì´ ê²€ìƒ‰ì´ ì•ˆëœë‹¤.
---


<div class="notice--info" markdown="1" style='font-size: 20px'>
**motivation:**  Create Your World: Lifelong Text-to-Image Diffusion  TPAMI 2024, Gan Sun et al., ë…¼ë¬¸ì„ ì´í•´í•´ë³´ì.
</div>

# self notes

### Research

* Methodology::
* Key Contructs::
	* IVs::
	* DVs::
	* Moderators::
	* Others::
* Key_Findings::
* Contributions::
* Limitations::

### Self Critique

* Critique
	* Strengths
	* Limitations

* How is it relevant to my research?
	* Relevant_topic::
	* Use::


> [!Cite]
> [1]  G. Sun, W. Liang, J. Dong, J. Li, Z. Dingì™€/ê³¼Y. Cong, â€œCreate Your World: Lifelong Text-to-Image Diffusionâ€, 2023ë…„ 9ì›” 8ì¼, _arXiv_: arXiv:2309.04430. ì ‘ê·¼ëœ: 2024ë…„ 7ì›” 25ì¼. [Online]. Available at: [http://arxiv.org/abs/2309.04430](http://arxiv.org/abs/2309.04430)

> [!Synth]
> **Contribution::**

> [!Md]
>
> **Author**:: Sun, Gan> 
>
> **Author**:: Liang, Wenqi> 
>
> **Author**:: Dong, Jiahua> 
>
> **Author**:: Li, Jun> 
>
> **Author**:: Ding, Zhengming> 
>
> **Author**:: Cong, Yang> 
>
> **Title**:: Create Your World: Lifelong Text-to-Image Diffusion
> **Year**:: 2023
> **Citekey**:: [[@2023-01-01-CreateYourWorldSunEtAl2023]]
> 
> >**Tags**:: Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition
> >**itemType**:: preprint
> >
> >
> >
> >
> >
> >
> >
> >
> >

> [!Link]
> [Sun ë“± - 2023 - Create Your World Lifelong Text-to-Image Diffusio.pdf](C:\Users\wys00\Zotero\storage\WU6N7PKX\Sun%20ë“±%20-%202023%20-%20Create%20Your%20World%20Lifelong%20Text-to-Image%20Diffusio.pdf)

> [!Abstract]
>
> abstract:: Text-to-image generative models can produce diverse high-quality images of concepts with a text prompt, which have demonstrated excellent ability in image generation, image translation, etc. We in this work study the problem of synthesizing instantiations of a userâ€™s own concepts in a never-ending manner, i.e., create your world, where the new concepts from user are quickly learned with a few examples. To achieve this goal, we propose a Lifelong text-to-image Diffusion Model (L2DM), which intends to overcome knowledge â€œcatastrophic forgettingâ€ for the past encountered concepts, and semantic â€œcatastrophic neglectingâ€ for one or more concepts in the text prompt. In respect of knowledge â€œcatastrophic forgettingâ€, our L2DM framework devises a task-aware memory enhancement module and a elastic-concept distillation module, which could respectively safeguard the knowledge of both prior concepts and each past personalized concept. When generating images with a user text prompt, the solution to semantic â€œcatastrophic neglectingâ€ is that a concept attention artist module can alleviate the semantic neglecting from concept aspect, and an orthogonal attention module can reduce the semantic binding from attribute aspect. To the end, our model can generate more faithful image across a range of continual text prompts in terms of both qualitative and quantitative metrics, when comparing with the related state-of-the-art models. The code will be released at https://wenqiliang.github.io/. Random samples Index Termsâ€”Lifelong Machine Learning, Stable Diffusion, Image Generation, Continual Learning. 
>

---

# Annotations 


![](images_for_zotero/2023-01-01-CreateYourWorldSunEtAl2023/images_for_zotero-1-x63-y447.png)





### 1 INTRODUCTION 

ê¸°ì¡´ì˜ ì œì•ˆëœ T2I diffusion modelì€ preset large-scale dataset and fixed conceptsì— ëŒ€í•´ì„œ ìƒì„± ëŠ¥ë ¥ì„ ëŠ˜ë¦¬ëŠ” ê²ƒì— ì´ˆì ì„ ë‘ì—ˆë‹¤. ì´ëŸ° ëª¨ë¸ë“¤ì€ userê°€ ì•ˆë³¸ ê°œë…ì— ëŒ€í•´ì„œ ì¼ë°˜í™”í•  ìˆ˜ ì—†ë‹¤.

ì´ëŸ° ëª¨ë¸ë“¤ì— ëŒ€í•´ì„œ, ìœ ì €ì˜ 'loved dog' ì— ëŒ€í•´ì„œ, ì‚¬ì§„ì„ ìƒì„±ì‹œí‚¤ê³  ì‹œë‹¤ë©´, diffusion modelì„ ë‹¤ì‹œ í•™ìŠµì‹œí‚¤ë©´ ë  ê²ƒì´ë‹¤. ê·¼ë° ë¬¸ì œëŠ”, ë‹¤ìŒë‚ ì— Jamesë¼ëŠ” ì¹œêµ¬ ì´ë¦„ì„ ì¶”ê°€í•˜ê³  ì‹¶ë‹¤ë©´, Jamesë¥¼ datasetì— ì¶”ê°€í•´ì„œ fine-tuningì„ large-scale diffusion modelì— í•˜ë©´, computational costë‘ fine-tune processë¥¼ ë¹„ì‹¤ìš©ì ìœ¼ë¡œ ë§Œë“ ë‹¤. ì™œëƒí•˜ë©´ userëŠ” pre-trained large-scale diffusion modelì—ì„œ ìƒˆë¡œìš´ ê°œë…ë“¤ì„ ê³„ì† ë°°ì›Œì•¼ í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” Lifelong T2I diffusion modelì„ ì œì•ˆí•œë‹¤. ì´ ëª¨ë¸ì€ ì—°ì†ì ìœ¼ë¡œ, userì˜ multiple personalized conceptì„ í•™ìŠµí•˜ë©´ì„œ, ê¸°ì¡´ì˜ distinctive í•œ featureë“¤ì€ ë³´ì¡´í•˜ë„ë¡ í•˜ëŠ” ë°©ë²•ì´ë‹¤.

lifelong T2I diffusion ì„¸íŒ…ì˜ ë¬¸ì œë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.

1. **catastrophic forgetting**
	1. **prior knowledge forgetting:**
		- personalizedí•œ ê°œë…ì„ ë°°ìš°ê³  ë‚˜ë©´, originalí•œ dataì— ëŒ€í•´ì„œ diffusion modelì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ì¼ê´€ë˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. ì´ê²Œ ë¬´ìŠ¨ ë§ì´ëƒë©´, ë‚´ê°€ ìƒˆë¡œ ë°°ìš°ëŠ” moongateë¼ëŠ” ê°œë…ê³¼ ê¸°ì¡´ì˜ diffusionì´ ì•Œê³  ìˆì—ˆë˜ moonê³¼ ê°œë…ì´ ë‹¤ë¥¼ ë•Œ, moonì˜ ê°œë…ì´ ì¼ê´€(?)ë˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. (moonì˜ ê°œë…ì´ moongateì— ì˜í•´ì„œ ë‹¬ë¼ì§€ëŠ” ê²ƒ.) ê·¸ë˜ì„œ moonì´ë¼ëŠ” ê°œë…ì„ forgetí•´ì•¼ í•˜ëŠ” ë¬¸ì œì¸ ê²ƒì´ë‹¤.
	2. personalized knowledge forgetting:
		- userì— ì˜í•´ì„œ ì£¼ì–´ì§„ conceptë“¤ì„ ë˜ëŒë¦´ ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì˜ë¯¸í•œë‹¤. personalized conceptì„ ë°°ì›Œë„, ì´ì „ì— originalí•œ ê°œë…ì„ additional training data ì—†ì´ ë‹¤ì‹œ ë³µì›í•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.

2.  **catastrophic neglecting**
		í•˜ë‚˜ì˜ promptë¥¼ ê°€ì§€ê³ , target promptì˜ semantic catastrophic neglectë¥¼ í”¼í•˜ë©´ì„œ, ì—¬ëŸ¬ê°œì˜ personlized ê°œë…ë“¤ì„ ì •í™•íˆ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì˜ë¯¸í•œë‹¤. --> e.g. "a photo of pet dog with James" ì˜ ê²½ìš°, Jamesê°€ ì•ˆë‚˜ì˜¬ ìˆ˜ ìˆìŒ. catastrphic neglect.
			e.g., concept neglectingì˜ ì˜í–¥ì„ ê·¹ë³µí•˜ëŠ” ê²ƒì€ stable diffusionì˜ ì£¼ìš” ê³¼ì œ ì¤‘ í•˜ë‚˜ë‹¤.
		**attribute neglecting / catastrophic homogenization:** ë‘ê°œì˜ ë¬¼ì²´ê°€ ë¹„ìŠ·í•œ ì‹œê°ì  íŠ¹ì§•ì„ ê°€ì§€ê³  ìˆìœ¼ë©´, stable diffusionì€ attribute neglectingì„ í•˜ê²Œ ë  ìˆ˜ ìˆë‹¤. --> e.g. "photo of a dog and cat": two dogs, two catsê°€ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ. (ì´ê±°ëŠ” ë¬´ì‹œëŠ” ì•„ë‹ˆê³ , íŠ¹ì„±ì„ ë¬´ì‹œí•˜ëŠ” ê²ƒì´ì–´ì„œ, attribute neglectingì¸ë“¯.)


ê²°êµ­ ìš°ë¦¬ëŠ” continual personalized T2I lifelong diffusion frameworkì„ ì œì•ˆí•œë‹¤. ìš°ë¦¬ ëª¨ë¸ì€ ìƒˆë¡œìš´ ê°œë…ì˜ sequenceë¥¼ ì¶”ê°€í•˜ëŠ” ê³¼ì •ì´ memory efficient, computation effectiveí•˜ë‹¤.

ê·¸ëŸ¬ë‚˜ ìœ„ì˜ challengeë“¤ì„ tackleí•˜ê¸° ì „ì—, ê¸°ì €ì— ìˆëŠ” ê°€ì •(ê°€ì„¤)ì´ ìˆë‹¤.

> [!Highlight]
> the underlying hypothesis is that the pre-trained diffusion model can be updated with a few samples of the new concept, which motivates we design a Lifelong text-to-image Diffusion Model (L2DM) via considering **lifelong-task learning and multi-concept generation.** (2 )
>  


ë” êµ¬ì²´ì ìœ¼ë¡œ ì–˜ê¸°í•˜ìë©´, ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì„ ê°œë°œí•˜ì˜€ë‹¤.


1. **task-aware memory enhancement module** (prior knowledge ë³´ì¡´ì„ ìœ„í•´ì„œ)
2. **elastic concept distillation module** (personalized knowledge ë³´ì¡´ì„ ìœ„í•´ì„œ)
ì´ ëª¨ë“ˆë“¤ì€ lifelong generation task sequeunceì— ëŒ€í•´ì„œ, 


ì´ ë‘ê°€ì§€ ëª¨ë“ˆë“¤ì€ 'prior conceptê³¼ learned personalized concept'ë¥¼ ë°°ìš°ë©´ì„œ, ê·¸ conceptë“¤ì˜ ê°ê°ì˜ íŠ¹ì§•(identifying features)ë“¤ì´ ì˜ ë³´í˜¸ë  ìˆ˜ ìˆê²Œ ë§Œë“¤ì–´ì¤€ë‹¤.

ë˜í•œ 

3. **concept attnetion artist module** (concept-neglectingì„ ì™„í™”í•˜ê¸° ìœ„í•´ì„œ)ê³¼
4. **orthogonal attention module** (attribute-neglectingì„ ì™„í™”í•˜ê¸° ìœ„í•´ì„œ)

ì˜ ìƒˆë¡œìš´ moduleë“¤ì„ ì œì•ˆí•œë‹¤. ì´ ë‘ê°œì˜ moduleë“¤ì€ ëª¨ë‘” multiple conceptë“¤ì´ imageì— ë“¤ì–´ê°€ë„ë¡ í•˜ëŠ” moduleë“¤ì´ë‹¤.

userê°€ ì£¼ëŠ” ìƒˆë¡œìš´ conceptì´ë‚˜ genration taskë¥¼ ì£¼ì–´ì¡Œì„ ë•Œ, ìš°ë¦¬ì˜ lifelong learning mechanismì€ **rainbow-memory bank strategy**ë¥¼ í†µí•´ì„œ **elasticí•˜ê²Œ concept knowledgeë¥¼ ë°°ìš¸ ìˆ˜ ìˆë‹¤.**
ê·¸ëŸ° ë‹¤ìŒ, attention artist moduleì„ í†µí•´ì„œ ìƒì„±ëœ ì´ë¯¸ì§€ì— ëŒ€í•œ semaintic informationì„ ë³€ê²½ì‹œí‚¨ë‹¤.

ë˜í•œ extensive experimentsë¥¼ í†µí•´ì„œ SOTAì™€ì˜ ë¹„êµë„ í–ˆë‹¤.

L$^2$DM ì˜ noveltyëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
- L$^2$DM senarioë¥¼ ì²˜ìŒ exploreí•œë‹¤.
- catastrhopic forgettingì„ ìœ„í•´ì„œëŠ”, task aware memory enhancement moduleê³¼ elastic concept distillation moduleì„ ì‚¬ìš©í•œë‹¤. ì´ ëª¨ë“ˆë“¤ì€ ê°ê° prior knowledgeë¥¼ rehearsal ì¸¡ë©´ì—ì„œ ê³µê³ í™” ì‹œí‚¬ ìˆ˜ ìˆê³ , elasticí•˜ê²Œ personalized knowledgeë¥¼ distill í•  ìˆ˜ ìˆë‹¤.
- catastrophic neglecting issueì— ëŒ€í•´ì„œëŠ”, novel concept attention artist module and an orthogonal attention moduleì„ ì œì•ˆí•œë‹¤ (multiple user-specific conceptsì„ ìƒì„±í•  ë•Œ ë°œìƒë˜ëŠ” ë¬¸ì œ). ê·¸ë˜ì„œ ì´ë¥¼ í†µí•´ì„œ, text promptì— ìˆëŠ” described semanticsë¥¼ ë‹¤ ì„¤ëª…í•  ìˆ˜ ìˆê²Œ ëœë‹¤.





### 2 RELATED WORK 

ì´ ë¶€ë¶„ì€ 1. Lifelong Machine learningê³¼ 2. T2I diffusionì— ëŒ€í•´ì„œ ê°„ëµí•˜ê²Œ ë‹¤ë¥¸ë‹¤.

#### 2.1 lifelong Machine Learning

lifelong machine learningì€ ì„ íƒì  ì •ë³´ë¥¼ ì´ì „ task clusterì—ì„œ í˜„ì¬ ìƒˆë¡œìš´ taskë¡œ transfer ì‹œí‚¤ë ¤ê³  í•˜ëŠ” ê²ƒì´ë‹¤. ë˜ëŠ” ë³€í•˜ì§€ ì•ŠëŠ” ì§€ì‹ì„ ì „ì´ì‹œí‚¤ë ¤ê³  í•˜ëŠ” ê²ƒì´ë‹¤. LMLì˜ main targetì€ non-stationary data streamì„ íŒŒê´´ì ì´ì§€ ì•Šìœ¼ë©´ì„œ, í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì´ë‹¤.

[12] G. Sun, Y. Cong, Y. Zhang, G. Zhao, and Y. Fu, â€œContinual multiview task learning via deep matrix factorization,â€ IEEE Transactions on Neural Networks and Learning Systems, pp. 1â€“12, 2020. ì´ ë…¼ë¬¸ì€ cross-domain lifelong learning framework for reinforcement learning. ì„ ì œì•ˆ.

[13] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska et al., â€œOvercoming catastrophic forgetting in neural networks,â€ Proceedings of the national academy of sciences, vol. 114, no. 13, pp. 3521â€“3526, 2017.  ì´ ë…¼ë¬¸ì€ EWC modelì„ ê°œë°œ, FIMì˜ diagonalì„ ì´ìš©.

[14] Z. Li and D. Hoiem, â€œLearning without forgetting,â€ in European Conference on Computer Vision, 2016, pp. 614â€“629. ì´ ë…¼ë¬¸ì€ LwF ê°œë°œ, convolutional neural networkì— ì ìš©. which could retrain the neural networks, which could retrain the neural networks without using the past task data.

[15] J. Yoon, E. Yang, J. Lee, and S. J. Hwang, â€œLifelong learning with dynamically expandable networks,â€ in Sixth International Conference on Learning Representations. ICLR, 2018. ì´ ë…¼ë¬¸ì€ novel deep networkì„ ì œì•ˆ, ë™ì ìœ¼ë¡œ network capacityë¥¼ ê²°ì •. taskì˜ sequenceê°€ ì˜¬ ë•Œ. --> DEN.

[16] S. Yan, J. Xie, and X. He, â€œDer: Dynamically expandable representation for class incremental learning,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 3014â€“3023. ì´ ë…¼ë¬¸ì€ learnable feature extractorë¥¼ ì¶”ê°€. ì´ì „ì— ë°°ìš´ ê²ƒì€ freeze. --> DER.


[17] A. Douillard, A. RamÃ©, G. Couairon, and M. Cord, â€œDytox: Transformers for continual learning with dynamic token expansion,â€ in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 9285â€“9295. ì´ ë…¼ë¬¸ì€ transformerì¸ë° dynamic expansion of task tokens in CL paradigm. --> DyTox.

ê°€ì¥ ìµœê·¼ì— ì œì•ˆëœ discussionì—ì„œ, ê·¼ë° T2Iì—ì„œ ì–´ë–»ê²Œ CLì„ í• ì§€ì— ëŒ€í•œ ë…¼ì˜ëŠ” ì—†ìŒ. ê·¸ë˜ì„œ ìš°ë¦¬ workì´ pioneering workì„.

#### 2.2 T2I diffusion,

diffusion modelì€ remarkable generative power in various fields ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤. pretrained CLIPì„ ì´ìš©í•´ì„œ, text inputì„ condition vectorë¡œ ë°”ê¿”ì„œ, image generationì— ë§ì€ ì„±ê³µì ì¸ applicationë“¤ì„ ë³´ì—¬ì¤Œ.

ì˜ˆë¥¼ ë“¤ì–´ì„œ, 
- GLIDE: represents a text-guided diffusion modelì¸ë°, image generationê³¼ editing ëŠ¥ë ¥ì„ ë³´ì—¬ë‘ .
- Imagenì€ image generationì„ ìœ„í•´ì„œ classifier free guidanceì™€ pretrained large language modelì„ ì ìš©í–ˆë‹¤.
- Stable diffusionì€ VQ-GANì„ ì‚¬ìš©í•´ì„œ, pixel spaceì—ì„œ latent spaceë¡œ representation spaceë¥¼ mappingì‹œì¼°ë‹¤. ì´ë¥¼ í†µí•´ì„œ computational burdenì„ ì¤„ì¼ ë¿ë§Œ ì•„ë‹ˆë¼, ì˜ ì••ì¶•ëœ semantic featureë“¤ê³¼ visual patternë“¤ì„ í™œì„±í™”ì‹œì¼°ë‹¤.
- StructureDiffusionê³¼ Attend-and-excite ë…¼ë¬¸ ë‘ê°œëŠ” ë³‘ë ¬ì ìœ¼ë¡œ 'synthetic image of pretrained stable diffusionì—ì„œ ë°œìƒí•˜ëŠ” catastrophic neglect' ë¬¸ì œì˜ riskë¥¼ ì •ì˜í•˜ê³ , ì´ riskë¥¼ ì™„í™”í•˜ëŠ” ë°©ë²•ì„ ì†Œê°œí•œë‹¤. ì´ ì™„í™”í•˜ëŠ” ë°©ë²•ì€, structured guidanceë‚˜ refining the cross-attention unit í†µí•´ì„œ, ì´ riskë¥¼ ì™„í™”í•œë‹¤.

#### 2.3 Personalized T2I diffusion

user ë³¸ì¸ì˜ conceptë“¤ê³¼ aligní•  ìˆ˜ ìˆëŠ” ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ, stable diffusionì˜ potentialì„ í™œìš©í•˜ê¸° ìœ„í•´ì„œ, **fine-tuning**ì„ í†µí•œ customizationê³¼ personalization ì— ì´ˆì ì„ ë‘ì—ˆë‹¤. ê¸°ì¡´ ì—°êµ¬ëŠ”.

ì˜ˆë¥¼ ë“¤ì–´ì„œ, 
- DreamBoothëŠ” class-specific prior preservation lossë¥¼ í†µí•´ì„œ, prior knowledgeë¥¼ ë³´í˜¸í•˜ê³ , stable diffusionì˜ ëª¨ë“  paramterë¥¼ fine-tuningí•œë‹¤.
- ëŒ€ì¡°ì ìœ¼ë¡œ, Textual Inversionì€ ì˜¤ì§ ìƒˆë¡œìš´ conceptì„ ìœ„í•œ word vectorë¥¼ í•™ìŠµí•˜ëŠ”ë°ì— ì´ˆì ì„ ë‘”ë‹¤.
- Custom Diffusionì€ íŠ¹ì • attention layerì— ëŒ€í•´ì„œë§Œ ì œí•œì ì¸ parameter ìˆ˜ë§Œ ì„ íƒì ìœ¼ë¡œ fine-tuneì„ í•œë‹¤. ê·¸ë˜ì„œ ë¹ ë¥¸ tuningì„ ì´‰ì§„í•œë‹¤.
- SVDiffëŠ” SVDë¥¼ Stable diffusionì˜ weight matrices ì‚¬ìš©í•´ì„œ, personalized datasetì— ëŒ€í•´ì„œ íš¨ê³¼ì ì¸ tuningì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ , ì´ëŸ° ëª¨ë¸ë“¤ì€ **past dataë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³ ** ì—°ì†ì ìœ¼ë¡œ ìƒˆë¡œìš´ T2I duffusion taskë¥¼ **í•™ìŠµí•  ìˆ˜ ì—†ë‹¤.**




## 3 LIFELONG TEXT-TO-IMAGE DIFFUSION MODEL (L2DM) 


ì´ sectionì—ì„œëŠ” ifelong T2I diffusion methodë¥¼ ë‹¤ì–‘í•œ ì¸¡ë©´ì—ì„œ ì†Œê°œí•  ê²ƒì´ë‹¤. ë¨¼ì € T2I diffusion modelì— ëŒ€í•œ backgroundë¥¼ 3.1 ì—ì„œ ì†Œê°œí•˜ê³ ,

lifelong diffusion frameworkì˜ problem definitionì„ 3.2ì—ì„œ ë‹¤ë£° ê²ƒì´ê³ , 

3.3ì—ì„œ ìš°ë¦¬ê°€ ì œì•ˆí•œ ëª¨ë¸ì„ ë³¼ ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ì´ ì œì•ˆëœ ëª¨ë¸ì´, ì–´ë–»ê²Œ continual concept learningì„ ì˜ í•  ìˆ˜ ìˆëŠ”ì§€, ê·¸ë¦¬ê³  multiple conceptsë¥¼ ì˜ ìƒì„±í•  ìˆ˜ ìˆëŠ”ì§€ ë³¼ ê²ƒì´ë‹¤.



### 3.1 Revisit Text-to-Image Diffusion Model 

#### stable diffusion
Stable diffusionì€ generated imageë¥¼ ë‹¨ê³„ì ìœ¼ë¡œ ì •ì œë¥¼ í•˜ëŠ” ê³¼ì •ì„ ê±°ì¹œë‹¤. multiple iterationì„ í†µí•´ì„œ. ì´ê²ƒì„ í†µí•´ì„œ, textì™€ imageê°„ semantic gapì„ ì—°ê²°í•œë‹¤. latent encoder ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-4.png)ì™€ decoder ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-5.png)ë¥¼ ì‚¬ìš©í•´ì„œ, LDMs(stable diffusion)ì€ representationì„ low-dimensional í•˜ê³  enriched-semantic latent spaceì—ì„œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤.
ë” êµ¬ì²´ì ìœ¼ë¡œ, T2I generation taskëŠ”, LDMsì˜ ë³€ì´ë¡œ, text prompt ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-7.png)ì™€ initial noise map ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-8.png)ë¥¼ ê°€ì§€ê³ , 

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-1.png) ì´ë¯¸ì§€ xë¥¼ ì´ë ‡ê²Œ ë§Œë“¤ê²Œ ëœë‹¤. ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-2.png)  ëŠ” diffusion modelì„ ì˜ë¯¸í•˜ê³ , ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-3.png) ì—ì„œ PëŠ” CLIPì˜ text encoderì´ë‹¤.

(ì¦‰ promptë¥¼ CLIP text encoderì— í†µê³¼í•˜ë©´ conditioned vector cê°€ ë‚˜ì˜¤ê³ , cì™€ timestep të¥¼ diffusion modelì— ë„£ì–´ì¤€ ê²ƒì„ VQ-VAEì˜ decoderì— í†µê³¼ì‹œí‚¤ë©´ ì´ë¯¸ì§€ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ê²ƒ.)

ê·¸ë˜ì„œ, formalí•˜ê²Œ diffusion modelì€ ì–´ë–»ê²Œ trainingì´ ë˜ëƒí•˜ë©´, 
![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-9.png)
ì´ë ‡ê²Œ noise imageë¥¼ denoiseí•´ì„œ, denoised outputì„ ë‚´ë„ë¡ í•™ìŠµì„ í•˜ëŠ” ê²ƒì´ë‹¤.

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-10.png) :  corresponding pairs of image latents and text embeddings, 
![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-11.png): a latent code obtained from encoder E, 
![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-12.png) : the corresponding model parameters. 
![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-13.png) : which will be omitted in the following sections for brevity.

#### Dreambooth

DreamboothëŠ” T2I modelì„ personalizeí•˜ë ¤ê³  í–ˆë‹¤. fine-tuning í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ.

ì˜ˆë¥¼ ë“¤ì–´ì„œ, pretrained T2I diffusion modelì€ personalized datasetìœ¼ë¡œ fine-tuing ë  ìˆ˜ ìˆë‹¤. **í•œ 3~5ê°œì˜ image-text pair datasetìœ¼ë¡œ.**

ê·¸ë˜ì„œ PDM (Personalized T2I Diffusion Model)ì€ equivalaent text promptë¥¼ í†µí•´ì„œ, target conceptì˜ unprecedented imageë¥¼ ìƒì„±í•  ìˆ˜ ìˆê²Œ ëœë‹¤. --> **ì´ë•Œ personalized modelì˜ prior knowledgeëŠ” ì•ˆë³€í•œ ìƒíƒœë¡œ ìˆê²Œ ëœë‹¤.**

ì´ê±°ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œ, DreamboothëŠ” class-spedific prior preservation lossë¥¼ ì œì•ˆí•œë‹¤. ì´ lossëŠ” ìƒˆë¡œìš´ conceptì— ëŒ€í•´ì„œ overfittingì— ì €í•­í•˜ê¸° ìœ„í•´ì„œ, ë˜í•œ prior knowledgeë¥¼ ë³´ì¡´í•˜ê¸° ìœ„í•´ì„œ ì“´ë‹¤. 

training objectiveëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-14.png)

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-15.png) ëŠ” stable diffusionì— ì˜í•´ì„œ ë§Œë“¤ì–´ì§„ ì´ë¯¸ì§€ë¡œë¶€í„° encoding ëœ latent spaceë¥¼ ì˜ë¯¸í•˜ê³ , ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-16.png) ëŠ” ì´ì— ëŒ€ì‘ë˜ëŠ” text conditionì´ê³ , ëŒë‹¤ëŠ” ë‘ë²ˆì§¸ í•­ì˜ backpropagationì„ controlí•˜ê¸° ìœ„í•œ í•­ì´ë‹¤.

(ê°œì¸ì ìœ¼ë¡œ ì •ë¦¬: ê·¸ë‹ˆê¹ ë‘ë²ˆì§¸ termì€ ê¸°ì¡´ì— í•™ìŠµëœ diffusion modelì„, ì˜ˆë¥¼ ë“¤ì–´ì„œ \[V\] (unique identifier) dog (dogëŠ” unique identifierì˜ ìƒìœ„ ê°œë…) ì„ í•™ìŠµí•˜ê³ ì í•œë‹¤ë©´, dogê³¼ ê·¸ì— í•´ë‹¹í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ì™•ì°½ ë§Œë“¤ì–´ë†“ê³ , ì¦‰ 'a photo of a dog'ì˜ promptë¥¼ ì¤˜ì„œ ì´ë¯¸ì§€ë¥¼ ì—„ì²­ ë§Œë“¤ì–´ë†“ê³ ,ê·¸ conditionì„ ê³„ì† í•™ìŠµí•˜ëŠ” ê²ƒ. ì™œ ê¸°ì¡´ì˜ ì§€ì‹ì„ ê¹Œë¨¹ì§€ ì•Šìœ¼ë ¤ê³ . ê·¸ë˜ì„œ ëŒë‹¤ê°€ ì‘ì„ìˆ˜ë¡, ë” few-shot sampleì— overfittingí•˜ê²Œ ë˜ê³ , í´ìˆ˜ë¡ í•™ìŠµí•  ì´ë¯¸ì§€ì™€ ë©€ì–´ì§€ê²Œ ë¨.)

personalized T2I diffusion senarioëŠ” ê·¸ëŸ¬ë‚˜ continually learning new concepts without repeatly using past traninig dataëŠ” ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤.



### 3.2 Problem Definition for Lifelong Text-to-Image Diffusion 

#### ë¬¸ì œ ì •ì˜:
generation tasks: ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-17.png)

datasets: ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-18.png)
k-th generation taskëŠ” ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-19.png) ê°œì˜ pairë¡œ êµ¬ì„±ë¨. ì–´ë–¤ pairëƒë©´, concept image ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-20.png) ì™€ prompt ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-21.png) ë¡œ êµ¬ì„±ì´ ë˜ì–´ ìˆëŠ” ê²ƒ. ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-22.png) ëŠ” taskì˜ ê°œìˆ˜ë¥¼ ì˜ë¯¸, ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-24.png)ëŠ” 3ì—ì„œ 5ê°œë‹¤. ì´ paperì—ì„œëŠ”.

<br>

ì´ì „ì˜ recent prior-based personalized generation ë°©ë²•ë“¤ê³¼ ë‹¤ë¥´ê²Œ, ìš°ë¦¬ëŠ” consecutive specific conceptsë¥¼ lifelongí•˜ê²Œ ë°›ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìƒê°í•˜ê³  ìˆë‹¤. íŠ¹íˆ **ê°ê°ì˜** concept generation task ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-26.png) ê°€ ì•„ë˜ ì‹ (2)ë¡œ í‘œí˜„ë  ìˆ˜ ìˆëŠ” ê²ƒì´ë‹¤.

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-25.png)




ê°ê°ì˜ timestepì— ëŒ€í•´ì„œ,  kë²ˆì¨° taskê°€ concept image ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-20.png) ì™€ prompt ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-21.png) ì˜ pairê°€  ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-19.png) ê°œì˜ parië¥¼ ê°€ì§€ê³  lifelong T2I diffusionì— uesrì— ì˜í•´ì„œ í•™ìŠµì´ ë  ë•Œ, ì´ ì‹œìŠ¤í…œì€ ìƒˆë¡œìš´ conceptì„ ë°°ìš°ë©´ì„œ, ìƒˆë¡œìš´ conceptê³¼ ì´ë¯¸ ë°°ìš´ conceptì„ íš¨ìœ¨ì ìœ¼ë¡œ ì˜ ìœµí•©ì‹œì¼œì•¼ í•œë‹¤. 

privacy leakageì™€ limited memoryë¥¼ ìƒê°í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, **data pairë“¤ì´ í˜„ì¬ taskì—ë§Œ ì“¸ ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•˜ë©´ (== ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-27.png))** ***ê·¸ëŸ¬ë©´ lifelong T2I diffusion ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì í ìˆ˜ ìˆë‹¤.***

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-29.png)

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-30.png): **image latents ì™€ text embeddingì˜ pair, for k-th generation task**

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-31.png): **prior pairs generated by pretrained model for the k-th generation task.**


(Dreamboothì—ì„œ, prior preservation lossë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒ. ì´ê±°ëŠ” ê²°êµ­ Î»ğœ†ëŠ” prior-preservation í•­ì˜ ìƒëŒ€ì  ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ëŠ” ê°’ì´ë‹¤. ì €ìë“¤ì€ lossê°€ ë‹¨ìˆœí•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  overfittingê³¼ language-drift ë¬¸ì œë¥¼ ê·¹ë³µí•˜ëŠ” ë° íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ë°œê²¬í–ˆë‹¤. 200 ì´í•˜ì˜ epoch, learning rateÂ 10âˆ’510âˆ’5,Â Î»=1ğœ†=1ìœ¼ë¡œ ë‘ëŠ” ê²ƒì´ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‹¬ì„±í•˜ëŠ” ë° ì¶©ë¶„í•˜ë‹¤ê³  í•œë‹¤. í•™ìŠµ ê³¼ì •ì—ì„œ 200Ã—NÃ—ğ‘ê°œ ì´í•˜ì˜ â€œa [class noun]â€ ìƒ˜í”Œë“¤ì´ ìƒì„±ë˜ë©°,Â Nğ‘ì€ ì£¼ì œ ë°ì´í„°ì…‹ì˜ í¬ê¸°ë¡œ ì¼ë°˜ì ìœ¼ë¡œ 3~5ì´ë‹¤. í•™ìŠµ ê³¼ì •ì€ 1ê°œì˜ TPUv4ì—ì„œ15ë¶„ì´ ê±¸ë¦°ë‹¤.)[https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/dreambooth/](https://kimjy99.github.io/%EB%85%BC%EB%AC%B8%EB%A6%AC%EB%B7%B0/dreambooth/)

ê·¸ë˜ì„œ eq 3ì„ í†µí•´ì„œ, diffusion model paramter thetaë¥¼ ì–»ëŠ” ê²ƒì´ ëª©í‘œì¸ ê²ƒì´ë‹¤.
**êµ¬ì²´ì ìœ¼ë¡œ, ëª©í‘œì ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.** 
- **Generation Performance:** ê¸°ì¡´ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì—ì„œ conceptì„ drift ë˜ëŠ” neglect ì‹œí‚¤ì§€ë„ ì•Šì•„ì„œ, ì •í™•íˆ multiple personalized conceptì„ ìƒì„±í•˜ë„ë¡ ë§Œë“œëŠ” ê²ƒ.
- **Computation Efficiency:** ê¸°ì¡´ì˜ ë°©ë²•ë¡ ë“¤ë³´ë‹¤ ë” ë¹¨ë¦¬ ìƒˆë¡œìš´ conceptì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.
- **Lifelong Learning:** ê³„ì† continualí•˜ê²Œ userì˜ dataê°€ ë“¤ì–´ì˜¨ë‹¤ê³  í•˜ë”ë¼ë„, ìƒˆë¡œìš´ ê°œë…ë“¤ì„ ì˜ í•™ìŠµí•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.


**ì¦‰ eq3ì˜ loss functionìœ¼ë¡œ í•™ìŠµí•˜ë©´ì„œ, 3ê°€ì§€ targetì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ì„œ, **

**catastrophic forgettingê³¼ neglecting ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” 4ê°€ì§€ moduleë“¤ì— ëŒ€í•´ì„œ ì†Œê°œí•  ê²ƒì´ë‹¤.** 





### 3.3 The Proposed L$^2$DM Framework 

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-32.png)

figure 2ì— ì œì‹œëœ ê²ƒì²˜ëŸ¼, L$^2$DM ë¥¼ ìš°ë¦¬ëŠ” ì œì‹œí•œë‹¤. prior knowledgeë¥¼ ë³´ì¡´í•˜ë©´ì„œ, ìƒˆë¡œìš´ personalized generation taskë¥¼ ì—°ì†ì ìœ¼ë¡œ ë°°ìš°ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ” ê²ƒ.

#### catastrophic forgetting

catastrophic forgettingì˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ, ìš°ë¦¬ëŠ” task-aware memory enhancement module **(TAME)** ê³¼, elastic concept distillation **(ECD)** ë¥¼ ê³ ì•ˆí–ˆë‹¤.

**ì´ ë‘ê°€ì§€ moduleì€ 1. previous taskì˜ ì§€ì‹ì„ ë³´í˜¸í•˜ê³ , 2. ìƒˆë¡œ encounterí•œ taskì— ëŒ€í•´ì„œ íš¨ê³¼ì ìœ¼ë¡œ representational capacityë¥¼ í™•ì¥ ì‹œí‚¤ëŠ” ì—­í• ì„ ê°™ì´ ë™ì‹œì— ìˆ˜í–‰í•˜ê²Œ ëœë‹¤.**

#### catastrophic neglecting

L$^2$DMì˜ modelì˜ multi-concept inference stage ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ”, catastrophic neglect and homogenization ë¬¸ì œë¥¼ tackleí•˜ê¸° ìœ„í•´ì„œ, concept attention artist **(CAA)** ëª¨ë“ˆê³¼ orthogonal attention artist **(OAA)** moduleì„ ê°œë°œí–ˆë‹¤.

ì´ ë‘ê°€ì§€ moduleì€ each individual conceptì˜ representationì„ ë³´í˜¸í•˜ê³ , ê·¸ conceptë“¤ì˜ ê°ê°ì˜ íŠ¹ì§•(identifying features)ì—ì„œ ìƒê¸°ëŠ” catastrophic neglecting riskë¥¼ ì™„í™”í•˜ì—¬, ë‹¤ë¥¸ conceptì— ì˜í•´ì„œ í•´ë¡­ê²Œ ê°ì—¼ë˜ì§€ ì•Šë„ë¡ ë§Œë“ ë‹¤. **(ë¬´ì—‡ì´? individual conceptì˜ representationì´)**



#### 3.3.1 Lifelong-task Learning 

large-scale diffusion modelì—ì„œ, ì§€ì‹ì´ learned taskì—ì„œ new taskë¡œ ì´ë™í•  ë•Œ, ì€ catastrophic forgettingì„ ì–´ë–»ê²Œ ì™„í™”í•˜ëŠ”ê°€?

ê¸°ì¡´ì˜ lifelong learning problemê³¼ ë‹¤ë¥´ê²Œ, (LWF, EWC), ìš°ë¦¬ëŠ” ì´ workì—ì„œ  L$^2$DMì˜ ë‘ê°œì˜ distinctí•œ forgetting aspectsë¥¼ ê³ ë ¤í•´ì•¼ í•œë‹¤.

\1. **prior-forgetting**: ìƒˆë¡œìš´ personalized conceptsë¥¼ modelì— í†µí•©ì‹œí‚¬ ë•Œ, ê¸°ì¡´ì— ë°°ìš´ prior conceptë¥¼ ìƒì–´ë²„ë¦´ ìˆ˜ ìˆë‹¤.
\2. **personalized-forgetting**: ìš°ë¦¬ì˜ L$^2$DMì´ ì—°ì†ì ìœ¼ë¡œ  user personalized conceptsì„ ë°°ìš°ëŠ”ë°, ë” ì´ì „ì— ë°°ìš´ personalized conceptë„ ìŠí˜€ì§ˆ ìˆ˜ ìˆë‹¤.

ì´ lifelong-task learningì˜ ë‘ê°€ì§€ challengeë“¤ì„ tackleí•˜ê¸° ìœ„í•´ì„œ, ë‹¤ìŒ sectionì—ì„œ lifelong learning algorithmì„ ìì„¸íˆ ì„¤ëª…í•  ê²ƒì´ë‹¤.

\1. task-aware memory enhancement
\2. elsatic concept distillation

#### 3.3.1.1 Task-Aware Memory Enhancement (TAME): 

To tackle **prior-forgetting** when facing with a new user-specific concept, one naive approach is to retain or fine-tune the diffusion model ***via storing dataset of user-specific concepts.***

**--> ì–´ë–»ê²Œ prior forgettingì„ ê°€ì¥ ì˜ ì•ˆì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” sampleì„ ì°¾ëŠ” ê²ƒ.**
--> ê·¸ëŸ¬ë‚˜ ì´ ë¬¸ì œëŠ” computation, storage costê°€ ë„ˆë¬´ ë“¬.
--> ë˜ training data ê°€ì§€ê³  ìˆëŠ” ê²ƒì´ privacy ì¸¡ë©´ì—ì„œ ì¢€ ê·¸ëŸ¼.

ê·¸ë˜ì„œ eq2ì—ì„œ ì˜ê°ì„ ë°›ì•„ì„œ, prior-forgettingì„ ì¤„ì´ë©´ì„œ(2nd term) **personalized-forgetting ë³´ì¡´í•˜ë„ë¡, í•˜ê²Œ 'task-aware memory enhancement module'ë¥¼ ê³ ì•ˆí–ˆë‹¤.(1st term)**

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-33.png)

L$^2$DMì˜ memory enhancement moduleì€ ìœ„ì™€ ê°™ë‹¤.

Ë† : represents the input ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-34.png)is from short-term memory
hyperparameter Î¼ is utilized to reweight the loss function.


ìì„¸íˆ ì„¤ëª…í•˜ìë©´, Eq (4)ì˜ ì²«ë²ˆì¨° termì€ L2DMì„ ì§€ë„í•˜ëŠ” termì¸ë°, fine-grained memory bankë¥¼ ì‚¬ìš©í•œë‹¤. ì´ memory bankëŠ” learned personalized conceptsì— ëŒ€ì‘ë˜ëŠ” generated dataë‹¤.

ê¸°ì¡´ì˜ 3ë²ˆ ì‹ê³¼ ë‹¤ë¥¸ ì ì€ ì²«ë²ˆì¨° í•­ì¸ë°, STMì—ì„œ ì˜¨ë‹¤ëŠ” ê²ƒì´ ë‹¤ë¥´ë‹¤. memory bankì—ì„œ ì˜¤ëŠ”ë°, ê·¸ëŸ¬ë©´ ì–´ë–»ê²Œ STM, LTMë¥¼ ì–´ë–»ê²Œ store í•˜ëŠ”ê°€? ì´ê±°ëŠ” **Rainbow-Memory Bank Strategy** ì—ì„œ ë‹¤ë£¬ë‹¤.


#### 3.3.1.2 Rainbow-Memory Bank Stragegy: 

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-35.png)

ê²°êµ­ short term memoryëŠ” 
prior-forgettingì„ ì¤„ì´ë©´ì„œ(2nd term) 
**personalized-forgetting (1st term)ì´ first termì´ ë°”ë€ ê²ƒ.** ì´ê±°ë¥¼ ì¤„ì´ê² ë‹¤ëŠ” ê²ƒ.

STMëŠ” ì–´ë–»ê²Œ? 

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-36.png)


![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-37.png): short-term memory bank
![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-47.png): long-term memory bank
for current k-th task.

current k-th generation taskë¥¼ ë°°ìš¸ ë•Œ, STM bankëŠ” CLIP feature spaceì—ì„œ ë§Œë“¤ì–´ì§„ë‹¤. ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-39.png) ì´ 1ë¶€í„° k-1ê¹Œì§€ì˜ LTM bankì˜ guideë¥¼ ë°›ì•„ì„œ.

LTMëŠ” ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-40.png) ì´ë ‡ê²Œ êµ¬ì„±ë¨.
fëŠ” CLIP feature, pëŠ” text prompt

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-38.png) ëŠ” ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-41.png) ì´ë ‡ê²Œ CLIP featuresë¡œ êµ¬ì„±ë˜ì–´ìˆëŠ”ê±°ê³ .


LTM bankì—ëŠ” ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-42.png) ì´ STM bankì— ìˆëŠ” CLIP featuresë“¤ì„ diffusion modelì— ëŒë¦¬ë©´ ìƒê¸°ëŠ” ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ë§í•œë‹¤.


Considering the identifying characteristics for each past generation task, we define a score function to further cast the short-term memory bank ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-38.png):

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-43.png)


![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-44.png), ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-45.png) ëŠ” ê°ê° l-th taskì˜ a-th generated sample, l'-th taskì˜ b-th generated sampleì— í•´ë‹¹í•˜ëŠ” CLIP featureë¥¼ ì˜ë¯¸í•œë‹¤.

 ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-46.png) ì´ featureëŠ” LTM bankì¸ ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-48.png)ì•ˆì— ìˆëŠ” l-th taskì˜ i-th sampleì— ëŒ€í•œ featureë¥¼ ì˜ë¯¸.

sim(Â·) denotes the cosine similarity,

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-49.png) ëŠ” l'-th learned taskì— ëŒ€í•œ generated imageì˜ ì´ ìˆ«ì ë¥¼ ì˜ë¯¸í•˜ê³ ,  

Î² > 0 is a trading-off parameter.

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-50.png) ì´ first termì€ generated imageì˜ catastrophic forgetting degreeë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ì„œ ë§Œë“¤ì–´ì¡Œê³ ,

![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-51.png)
ë‘ë²ˆì§¸ termì€ LTM bankì˜ guidanceë¥¼ ì†Œê°œí•˜ê¸° ìœ„í•œ ì¤‘ì¶”ì  ì—­í• ì´ë‹¤.


ì´ scoreê°€ ë†’ì€ ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-52.png) ë¥¼ STM bankì¸ ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-54.png) ì—¬ê¸°ì— ë„£ì„ ê²ƒì´ê³ , LTM bank ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-55.png) ì´ê±°ëŠ” ![](../../../images/20240730-2024-01-01-CreateYourWorldSunEtAl2024TPAMI-56.png) ì´ê±¸ë¡œ recastë  ê²ƒì´ë‹¤.





## 4 EXPERIMENTS 




### 4.1 Datasets and Evaluation 




### 4.2 Implementation Details 




### 4.3 Comparison Evaluation 




### 4.3.1 Lifelong Single-concept Generation Results 




### 4.3.2 Lifelong Multi-concept Generation Results 




### 4.3.3 Ablation Study 




## 5 CONCLUSION 


---

