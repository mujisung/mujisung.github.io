---
layout: posts
title: 나만의 GPT를 만들어보자!
categories: AI # [coding, ]
tag: [GPT, transformers]  # tag가 추가됨.
author_profile: false # 글을 누르면 내 소개가 없어짐. true로 하면 얼굴이 나옴.
sidebar:      # 글을 누르면 목차가 나온다.
  nav: "counts" 
search: true     # false라고 하면 글이 검색이 안된다.
---

<div class="notice--info" markdown="1" style='font-size: 20px'>
**motivation:**  transformer와 pytorch workflow를 이해하기 위해서 나만의 GPT를 만들어보자!
</div>


강의영상: [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)


Andrej Karpathy가 2023년 1월에 만든 영상인 `Let's build GPT: from scratch, in code, spelled out.` 통해, transformer를 더 이해하고, pytorch workflow를 더 이해해 보려고 한다.

# 0. objectives

tiny sheakspeare dataset을 통해서, 한 character씩 출력하는 GPT를 만들어보려고 한다. 


# 1. read and explore the data

## 1. download tiny shakespeare dataset
``` python

!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/inpu.txt

```

## 2. open the text file

``` python

with open('input.txt', 'r', encoding='utf-8') as f:
    text = f.read()
```

## 3. print length of characters in text file and check the text 

``` python
print("length of dataset in characters: ", len(text))

print(text[:1000])
```

## 4. dataset에서 unique한 word 확인하는 방법

``` python
chars = sorted(list(set(text))) # 전체 dataset에서 유일한 character들의 list
vocab_size = len(chars)
print(''.join(chars)) 
print(vocab_size) # 65
```

# 2. tokenization-raw string(character)을 sequence of integer로 바꾸기 위해서.


## 1. string을 token으로 바꾸어주는 것.

``` python
# create a mapping from characters to integers
stoi = {ch: i for i, ch in enumerate(chars)}
itos = {i: ch for i, ch in enumerate(chars)}
encode = lambda s: [stoi[c] for c in s] # encoder는 string을 받아서, 그 token(여기서는 한개의 character)에 해당하는 integer들의 list를 return한다.
decode = lambda l: ''.join([itos[i] for i in l]) # decoder: integer의 list를 받아서, string을 출력한다.

print(encode("hii there"))
print(decode(endcode("hii there")))
# 를 하게 되면, list의 integer를, 다시 hi there로 바꾸어준다.
```
### tokenizer는, 많은 다른 방법들이 있다. 구글은 text to integer 스키마를 sentencepiece를 쓴다.

[**SentencePiece**](https://github.com/google/sentencepiece)는 sub-word units(tokens)를 사용하는데, 하나의 단어 전부를 encoding하는 것이 아니라, 또 각각의 character를 encoding하는 것은 또 아니다. **sub-word unit level**이다. openAI는, [**tiktoken**](https://github.com/openai/tiktoken) 이라는 라이브러리가 있는데, 이거는 [BPE(byte pair encoding tokenizer)](https://en.wikipedia.org/wiki/Byte_pair_encoding)인데, 이거는 **실제 GPT** 에서 사용되고 있는 방법이다.

``` python
import tiktoken
enc = tiktoken.get_encoding("hii there") # 이런 식으로 list of tokens를 받을 수 있다. 3개의 개수의 list가 나온다.
assert enc.decode(enc.encode("hello world")) == "hello world"
enc.n_vocab  # 50257 개의 token을 가진다.


# To get the tokeniser corresponding to a specific model in the OpenAI API:
enc = tiktoken.encoding_for_model("gpt-4")
```

결국 
small codebook size <--> long sequence list
large vocabulary size <--> short sequence list (\[43, 2, 5\])

이렇게 된다. 보통 sub-word encoding을 사용하는데, 안드레는 간단하게 하고 싶어서 character level tokenize한다고 한다.


## \2. shakespeare의 모든 data를 tokenize하자.


``` python
import torch
data = torch.tensor(encode(text), dtype=torch.long) # 모든 text를 encoding해서, tensor에 넣겠다.
print(data.shape, data.dtype) # torch.Size([1115394]) torch.int64
print(data[:1000]) # 1000개의 sequence를 보자.

```


## 3. train/validation split을 하자. 9:1로. (overfitting을 막기 위해서, 즉 정확히 90%의 data처럼 결과가 나오지 않도록.)

``` python
n = int(0.9*len(data))
train_data = data[:n]
val_data = data[n:]
```

# 3. data loader -> batches of chunks of data

text/integer sequences 를 transformer에 넣어서, pattern을 배울 수 있도록 하자. 

이때, transformer에 한번에 넣지 않을 것이다. 우리는 datset의 chuck들로 넣을 것이다. 이런 chunk들은, maximum length를 가진다. 
maximum length를, block_size라고 한다. 

## 1. maximum length를 8로 정하자.

``` python
block_size = 8
train_data[:block_size + 1]   # sequence 안에 있는 처음 9개 문자
```


## 2. transformer가 block_size 그 이상의 context를 갖지 못하도록 한다.

``` python
x = train_data[:block_size]   # transformer의 input
y = train_data[1:block_size+1]   # 다음 block_size 크기를 가지는 train_data 
for t in range(block_size):
  context = x[:t+1]
  target = y[t]
  print(f"when input is {context} the target: {target}")
```

``` output text
when input is tensor([18]) the target: 47
when input is tensor([18, 47]) the target: 56
when input is tensor([18, 47, 56]) the target: 57
when input is tensor([18, 47, 56, 57]) the target: 58
when input is tensor([18, 47, 56, 57, 58]) the target: 1
when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15
when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47
when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58
```

## 3. batch dimension을 맞춰야 한다.

chuncks of text를 sampling 할 때, transformer에 feed하기 전에, mutiple chuncks of text의 mini-batch가 하나의 tensor에 있게 된다. 
이렇게 하는 이유는, 병렬화를 하기 위함이다. 효율적으로 하기 위해서 병렬화 하기 위해서, 이 chunk는 독립적으로 수행되게 된다.

``` python
torch.manual_seed(1337)
batch_size = 4   # 병렬적으로 돌릴 독립적인 8개의 글자
block_size = 8   # maximum context length (8개의 글자)

def get_batch(split):
  data = train_data if split == 'train' else val_data   # split이 training split이면 train_data를 볼 것이다.
  ix = torch.randint(len(data) - block_size, (batch_size,) )  # batch_size(4) 개의, (0, 100000 - 8) 사이의 숫자 중에서, (4,) 처럼 data가 나오게 한다.
  x = torch.stack([data[i: i+block_size] for i in ix]) # i 번째에서 시작하는 starting block
  y = torch.stack([data[i+1 : i+block_size+1]for i in ix]) # x 다음 1개 이후에 나오는 8개의 character로 이루어진 sequence. 
  return x, y  # x, y는 4*8 tensor가 된다.

xb, yb = get_batch('train')
print('inputs:')
print(xb.shape)   # torch.Size([4,8])
print(xb)
print('targets:')
print(yb.shape)
print(yb)
  
print('----')

for b in range(batch_size): # batch dimension
    for t in range(block_size): # time dimension
        context = xb[b, :t+1]
        target = yb[b,t]
        print(f"when input is {context.tolist()} the target: {target}")

```

``` text output
inputs:
torch.Size([4, 8])
tensor([[24, 43, 58,  5, 57,  1, 46, 43],
        [44, 53, 56,  1, 58, 46, 39, 58],
        [52, 58,  1, 58, 46, 39, 58,  1],
        [25, 17, 27, 10,  0, 21,  1, 54]])
targets:
torch.Size([4, 8])
tensor([[43, 58,  5, 57,  1, 46, 43, 39],
        [53, 56,  1, 58, 46, 39, 58,  1],
        [58,  1, 58, 46, 39, 58,  1, 46],
        [17, 27, 10,  0, 21,  1, 54, 39]])
----
when input is [24] the target: 43
when input is [24, 43] the target: 58
when input is [24, 43, 58] the target: 5
when input is [24, 43, 58, 5] the target: 57
when input is [24, 43, 58, 5, 57] the target: 1
when input is [24, 43, 58, 5, 57, 1] the target: 46
when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
when input is [44] the target: 53
when input is [44, 53] the target: 56
when input is [44, 53, 56] the target: 1
when input is [44, 53, 56, 1] the target: 58
when input is [44, 53, 56, 1, 58] the target: 46
when input is [44, 53, 56, 1, 58, 46] the target: 39
when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
when input is [52] the target: 58
when input is [52, 58] the target: 1
when input is [52, 58, 1] the target: 58
when input is [52, 58, 1, 58] the target: 46
when input is [52, 58, 1, 58, 46] the target: 39
when input is [52, 58, 1, 58, 46, 39] the target: 58
when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
when input is [25] the target: 17
when input is [25, 17] the target: 27
when input is [25, 17, 27] the target: 10
when input is [25, 17, 27, 10] the target: 0
when input is [25, 17, 27, 10, 0] the target: 21
when input is [25, 17, 27, 10, 0, 21] the target: 1
when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39
```
결과가 이렇게 나오는데, 결국 y가 transformer의 마지막에서, loss를 계산하게 만드는 것이 된다.



# \4. baseline: bigram LM, loss, generation

이제 xb, transformer에 feed할 batch of inputs가 있게 되면, 
``` text
tensor([[24, 43, 58,  5, 57,  1, 46, 43],
        [44, 53, 56,  1, 58, 46, 39, 58],
        [52, 58,  1, 58, 46, 39, 58,  1],
        [25, 17, 27, 10,  0, 21,  1, 54]])
```
이거를 이제 NN에 feed해보자.

LM에서 가장 간단한 NN인 BiGram 모델로 시작하자.


## 1. bigram pytorch module

``` python
import torch
import torch.nn as nn
from torch.nn import functional as F
torch.manual_seed(1337)

class BigramLanguageModel(nn.Module):
  def __init__(self, vocab_size):
    super().__init__()
    # each token directly reads off the logits for the next token from a lookup table
    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # shape of row, col

  def forward(self, idx, targets):  # index, input x == idx
    # idx and targets are both (B,T) tensor of integers
    logits = self.token_embedding_table(idx) # (B,T,C) # 여기서 channel은 token을 몇차원의 벡터로 나타낼 것인지에 대해서, 그 벡터의 차원이 되는 것. 자세한 설명은 아래에.
    
    B, T, C = logits.shape
    logits = logits.view(B*T,C)
    tartgets = targets.view(-1)
    
    loss = F.cross_entropy(logits, targets) # loss는 prediction(logits)와 targets와의 CE다.
    
    return logits

m = BigramLanguageModel(vocab_size)
out = m(xb, yb)
print(out.shape)
```
을 하면 결과가 `torch.Size([4,8,65])` 가 나온다.

[cross entropy]


[nn.Embedding(1st=8, 2nd=3) 에 관한 설명](https://wikidocs.net/64779)

embedding_layer = nn.Embedding(num_embeddings=8, embedding_dim=3)
``` python
print(embedding_layer.weight)
Parameter containing:
tensor([[-0.1778, -1.9974, -1.2478],
        [ 0.0000,  0.0000,  0.0000],
        [ 1.0921,  0.0416, -0.7896],
        [ 0.0960, -0.6029,  0.3721],
        [ 0.2780, -0.4300, -1.9770],
        [ 0.0727,  0.5782, -3.2617],
        [-0.0173, -0.7092,  0.9121],
        [-0.4817, -1.1222,  2.2774]], requires_grad=True)


# 만약에 
embedding_layer = nn.Embedding(num_embeddings=8, embedding_dim=3)
tensor = torch.randint(0, 8, (4,8)) 
embedding_layer(tensor)
# 를 하게 되면, embedding_layer(tensor) 는 예를 들어서 tensor의 값이 7이 들어갔으면, 그 7에 해당하는 것을 [-0.4817, -1.1222,  2.2774] 이거로 바꾸어준다는 뜻. 
# 즉 tensor.shape를 하면 (4,8) 인데, 이 (4,8) 차원에서 각각의 정수 값을 3차원(embedding_dim) 차원 벡터 값으로 바꿔서, embedding_layer(tensor)의 shape은 (4,8,3)이 된다는 것이다.
```



## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```

## 

``` python


```
